{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text PreProcessing:","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-01-12T12:08:35.057891Z","iopub.execute_input":"2024-01-12T12:08:35.058612Z","iopub.status.idle":"2024-01-12T12:08:35.068710Z","shell.execute_reply.started":"2024-01-12T12:08:35.058563Z","shell.execute_reply":"2024-01-12T12:08:35.067334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Convert to Lower Case:","metadata":{}},{"cell_type":"markdown","source":"First thing that is very essential is, converting whole text to lower case becasue python is a case sensitive language and it will treat One and one as separate words.\n\nWe will start will perfom this on single row and then we will go for the whole dataset.","metadata":{}},{"cell_type":"code","source":"review= df['review'][2]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting to lower case \nreview.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#converting whole dataset to lower case\ndf['review']= df['review'].str.lower()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Remove Unimportatnt Things:","metadata":{}},{"cell_type":"markdown","source":"After you have converted the whole text to lower case, now remove all the things that are not important in the text like html tags (if you get the data by scraping and stuff).\n\nLike the dataset we are using has the tags in it, so we have to remove them: For this we will use 'regular expression' library.","metadata":{}},{"cell_type":"code","source":"import re\n#function for removing the html tags\ndef remove_html_tags(data):\n    #finding the tag pattern in the data\n    #it will catch anything in between <>\n    pattern= re.compile('<.*?>')\n    #now we will replace them with empty space\n    return pattern.sub(r'', data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we will use .apply() to apply the above function on the whole column of our dataframe\ndf['review']= df['review'].apply(remove_html_tags)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Remove URLs:","metadata":{}},{"cell_type":"code","source":"def remove_urls(data):\n    #we want to detect pattern starting with https and www as most/all the urls start with this \n    pattern= re.compile(r'https?://\\S+|www\\.\\S+')\n    return pattern.sub(r'', data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review']= df['review'].apply(remove_urls)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Remove Punctuations:","metadata":{}},{"cell_type":"markdown","source":"Python consider some particular symbols as Punctuations that we should remove in Text Preprocessing.","metadata":{}},{"cell_type":"code","source":"import string\nprint('Punctuations according to Python:', string.punctuation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Storing all the punctuations in the single variable:","metadata":{}},{"cell_type":"code","source":"exclude= string.punctuation\nexclude","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_punctuations(data):\n    #we will use maketrans function, that will replace character with '' if any is in exclude list\n    return data.translate(str.maketrans('', '', exclude))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review']= df['review'].apply(remove_punctuations)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Removing Stop Words:","metadata":{}},{"cell_type":"markdown","source":"Stop words are those words that help in sentence formation but do not actually contribute in the contextual meaning of the sentences.\n\nLike: and, for, it, I, me, my, we, our, ourselves etc\n\nWe will use 'nltk' library to remove these stop words.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"download them firstly:\n        \n        -import nltk\n        -nltk.download('stopwords')","metadata":{}},{"cell_type":"code","source":"#to get the english stop words\nenglish_stop_words= stopwords.words('english')\nenglish_stop_words","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def remove_stop_words(data):\n    #list where we will append all the words except stopwords\n    new_text= []\n    #splitting the sentences and iterating over words\n    for word in data.split():\n        #if the word is a stop word\n        if word in english_stop_words:\n            #append the empty space\n            new_text.append('')\n        else:\n            #else if it is not the stop word then append the word\n            new_text.append(word)\n            \n    x= new_text[:]\n    new_text.clear()\n    return \" \".join(x)\n            \n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review']= df['review'].apply(remove_stop_words)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Handling the Emojis:","metadata":{}},{"cell_type":"markdown","source":"We can use emoji package in the python, that will replace the emoji with its meaning in english.","metadata":{}},{"cell_type":"code","source":"import emoji","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"emoji.demojize(df['review'])","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['review'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Tokenization:","metadata":{}},{"cell_type":"markdown","source":"Tokenization is basically converting the text into small tokens or textual parts that can we words or sentences.\n\nHelps in while doing Feature Engineering or converting the words into numbers.","metadata":{}},{"cell_type":"markdown","source":"#### 1. Simple Tokenization:","metadata":{}},{"cell_type":"markdown","source":"If you simply want to do word or sentence tokenization, you can just use split function and get the tokenized output.","metadata":{}},{"cell_type":"code","source":"def word_tokenization(data):\n    return data.split()\ndf['simle_words_tokens']= df['review'].apply(lambda x: word_tokenization(x))\ndf['review'].head()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['simle_words_tokens'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similarly for the Sentence Tokenization you can split on '.' or new line.","metadata":{}},{"cell_type":"markdown","source":"#### 2. Using NLTK Library:","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize, sent_tokenize","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['nltk_word_tokenize']= df['review'].apply(lambda x: word_tokenize(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['nltk_word_tokenize'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets use sentence transformer:","metadata":{}},{"cell_type":"code","source":"df['nltk_sentence_tokenize']= df['review'].apply(sent_tokenize)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['nltk_sentence_tokenize'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Spacy:","metadata":{}},{"cell_type":"markdown","source":"Do this first:\n\n    -pip install spacy\n    -python -m spacy download en_core_web_sm","metadata":{}},{"cell_type":"markdown","source":"We will load small english dictionary from spacy:","metadata":{}},{"cell_type":"code","source":"import spacy\nnlp= spacy.load('en_core_web_sm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we will convert the text into doc firstly\ndoc= nlp('I am a Student!')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for token in doc:\n    print(token)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 8. Stemming:","metadata":{}},{"cell_type":"markdown","source":"Converting Word to its root form.\n\n        -Walking, Walked, Walks -----> Walk\n\n**Stemming is huge part of Information Retrieval Systems like We can take the example of google, if we search fishing, fisher on google it will have information of fish also.**","metadata":{}},{"cell_type":"markdown","source":"There are different stemming algorithms but for english we have Porters Stemming method:","metadata":{}},{"cell_type":"code","source":"from nltk.stem.porter import PorterStemmer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#initiate the porter stemming object\nps= PorterStemmer()\ndef stem_words(data):\n    #splitting the data and passing each word to stemmer and then returning it after joining as a sentence\n    return \" \".join([ps.stem(word) for word in data.split()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#applying function \ndf['porter_stemming']= df['review'].apply(lambda x: stem_words(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now see the difference in both","metadata":{}},{"cell_type":"code","source":"df['review'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['porter_stemming'].head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Main problem with stemming is that it does not care if the reduced root form exist in the english or not. Like little goes to little, movie goes to movi**","metadata":{}},{"cell_type":"markdown","source":"**This issue is solved by \"lemmitization\" in which root form will always exist in english**","metadata":{}},{"cell_type":"code","source":"import nltk\nnltk.download('wordnet')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nwordnet_lem= WordNetLemmatizer()\n\ndef lemmit(data):\n    #using nltk word tokenizer\n    words = word_tokenize(data)\n    return \" \".join([wordnet_lem.lemmatize(word) for word in words])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['lemmitized']= df['review'].apply(lemmit)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['review', 'porter_stemming', 'lemmitized']]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}